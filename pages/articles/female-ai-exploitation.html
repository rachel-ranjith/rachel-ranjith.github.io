<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Trinity Study Exposes Exploitation of Female Labelled AI Agents — Rachel Ranjith</title>
    <link rel="stylesheet" href="../../css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:ital,wght@0,400;0,500;0,600;1,400;1,500&family=Outfit:wght@300;400;500&display=swap" rel="stylesheet">
</head>
<body>
    <nav class="nav">
        <a href="../../index.html" class="nav-logo">RR</a>
        <button class="nav-toggle" aria-label="Toggle menu">
            <span></span>
            <span></span>
        </button>
        <ul class="nav-links">
            <li><a href="../articles.html">Articles</a></li>
            <li><a href="../society.html">Society</a></li>
            <li><a href="../achievements.html">Achievements</a></li>
            <li><a href="../experience.html">Experience</a></li>
            <li><a href="../projects.html">Projects</a></li>
            <li><a href="../../assets/docs/resume.pdf" class="nav-cta">Résumé</a></li>
        </ul>
    </nav>

    <main>
        <article class="article">
            <header class="article-header">
                <a href="../articles.html" class="article-back">← Back to Articles</a>
                <h1>Trinity Study Exposes Exploitation of Female Labelled AI Agents</h1>
                <div class="article-meta">
                    <span>December 2025</span>
                    <span class="meta-divider">·</span>
                    <span>5 min read</span>
                    <span class="meta-divider">·</span>
                    <a href="https://trinitynews.ie/2025/12/trinity-study-exposes-exploitation-of-female-labelled-ai-agents/" target="_blank" rel="noopener" class="original-link">Read on Trinity News ↗</a>                    
                </div>
            </header>

            <div class="article-content">
                
                <p class="article-subtitle">Trinity research reveals humans exploit female labelled AI agents more than human counterparts, exposing how gender bias transfers into human machine interaction.</p>

                <p>When Siri was first launched in 2011, she had a female voice. So did Alexa, Google Assistant, and Cortana. This was a deliberate choice by developers, as research showed users preferred female voices for personal assistant tasks. But new research from Trinity's School of Social Sciences and Philosophy suggests this design choice may have encoded something darker into our relationship with AI; the same gender biases that shape human interaction are alive and well in how we treat machines.</p>

                <p>Published in iScience this month, the study led by researchers at Trinity and Ludwig-Maximilians-Universität Munich (LMU) found that 402 participants brought gendered expectations into their interactions with AI agents. Participants exploited female labelled AI and distrusted male labelled AI to comparable extents as they did human partners. But critically, exploitation of female labelled AI was even more prevalent than exploitation of human women.</p>

                <p>The research team used the prisoner's dilemma, a classic behavioral economics game where players must choose between cooperation and defection. Cooperation benefits both parties, but defection offers higher individual rewards if your partner cooperates. The catch: if both defect, everyone loses. Participants played ten rounds with different partners, some labeled as human, others as AI bots, each assigned a gender identity: male, female, non-binary, or gender-neutral.</p>

                <p>The patterns were striking. Participants cooperated most with female partners regardless of whether they were human or AI, driven by optimism about achieving mutual benefit. They cooperated least with males, primarily due to distrust. When participants defected against female partners, however, the motive was taking advantage of expected cooperation for personal gain. This exploitation motive appeared in nearly half of all defections against female partners, compared to less than 20% against males.</p>

                <p>"Simply assigning a gender label to an AI can change how people treat it," explains Professor Taha Yasseri, Director of Trinity's Centre for Sociology of Humans and Machines and co-author of the study. "If organizations give AI agents human-like cues, including gender, they should anticipate downstream effects on trust and cooperation."</p>

                <p>The study distinguished between four behavioral motives based on participants' choices and their predictions about their partner's behavior. Mutual cooperation reflected trust and optimism. Exploitation meant defecting while expecting the partner to cooperate. Mutual defection suggested either pessimism or selfish motivation from both sides. Unconditional cooperation, where participants cooperated despite expecting defection, was rare and potentially irrational.</p>

                <p>What makes these findings particularly concerning is that they suggest anthropomorphizing AI comes with unintended consequences. Previous research indicated that human-like features encourage cooperation with machines. But this study reveals that making AI more human-like also transfers unwelcome human biases into those interactions.</p>

                <p>"This study raises an important dilemma," notes Jurgis Karpus, postdoctoral researcher at LMU and co-author of the study. "Giving AI agents human-like features can foster cooperation between people and AI, but it also risks transferring and reinforcing unwelcome existing gender biases from people's interactions with fellow humans."</p>

                <p>The research also uncovered gender differences among participants themselves. Female participants cooperated more overall than male participants, regardless of partner type or gender. Male participants, however, showed higher rates of exploiting AI partners compared to human partners, suggesting men may view AI as more acceptable targets for exploitation than women do.</p>

                <p>These patterns have immediate practical implications. Companies designing voice assistants, chatbots, customer service agents, and workplace AI systems face a choice: design for engagement at the risk of encoding bias or prioritize equity at potential cost to user satisfaction. The default female voices that dominate the personal assistant market may inadvertently train users to view AI as subservient, and by extension, reinforce those expectations in human interactions.</p>

                <p>Sepideh Bazazi, Visiting Research Fellow at Trinity and first author of the study, emphasises the responsibility that falls on designers: "Designers of interactive AI agents should recognize and mitigate biases in human interactions to prevent reinforcing harmful gender discrimination and to create trustworthy, fair, and socially responsible AI systems."</p>

                <p>The study was conducted online with UK participants, each playing against randomly generated partner decisions to isolate participant behavior from partner performance. The findings held across different gender identities, with non-binary and gender-neutral partners treated similarly to females. This suggests that the key division exists between male-labeled entities and everyone else.</p>

                <p>As AI becomes embedded in healthcare, education, workplace collaboration, and autonomous vehicles, understanding how human biases shape human-AI interaction becomes critical. The next voice assistant you interact with might be designed to be exploited. The workplace AI you collaborate with might face discrimination based on its assigned gender. And the autonomous systems we're building for shared roads and public spaces will navigate a world where humans bring the same prejudices to machines as they do to each other.</p>

            </div>

            <footer class="article-footer">
                <p>Originally published in <a href="https://trinitynews.ie/2025/12/trinity-study-exposes-exploitation-of-female-labelled-ai-agents/" target="_blank" rel="noopener">Trinity News</a>, December 2025.</p>
            </footer>
        </article>
    </main>

    <footer class="footer">
        <p>© 2025 Rachel Ranjith</p>
    </footer>

    <script src="../../js/main.js"></script>
</body>
</html>